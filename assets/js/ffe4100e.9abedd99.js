"use strict";(self.webpackChunkstarwhale_docs=self.webpackChunkstarwhale_docs||[]).push([[1830],{3905:(e,t,a)=>{a.d(t,{Zo:()=>m,kt:()=>c});var n=a(67294);function l(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function i(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function r(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?i(Object(a),!0).forEach((function(t){l(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):i(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function o(e,t){if(null==e)return{};var a,n,l=function(e,t){if(null==e)return{};var a,n,l={},i=Object.keys(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||(l[a]=e[a]);return l}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(l[a]=e[a])}return l}var s=n.createContext({}),d=function(e){var t=n.useContext(s),a=t;return e&&(a="function"==typeof e?e(t):r(r({},t),e)),a},m=function(e){var t=d(e.components);return n.createElement(s.Provider,{value:t},e.children)},p={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},u=n.forwardRef((function(e,t){var a=e.components,l=e.mdxType,i=e.originalType,s=e.parentName,m=o(e,["components","mdxType","originalType","parentName"]),u=d(a),c=l,h=u["".concat(s,".").concat(c)]||u[c]||p[c]||i;return a?n.createElement(h,r(r({ref:t},m),{},{components:a})):n.createElement(h,r({ref:t},m))}));function c(e,t){var a=arguments,l=t&&t.mdxType;if("string"==typeof e||l){var i=a.length,r=new Array(i);r[0]=u;var o={};for(var s in t)hasOwnProperty.call(t,s)&&(o[s]=t[s]);o.originalType=e,o.mdxType="string"==typeof e?e:l,r[1]=o;for(var d=2;d<i;d++)r[d]=a[d];return n.createElement.apply(null,r)}return n.createElement.apply(null,a)}u.displayName="MDXCreateElement"},61340:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>s,contentTitle:()=>r,default:()=>p,frontMatter:()=>i,metadata:()=>o,toc:()=>d});var n=a(83117),l=(a(67294),a(3905));const i={title:"Starwhale Dataset SDK"},r=void 0,o={unversionedId:"reference/sdk/dataset",id:"reference/sdk/dataset",title:"Starwhale Dataset SDK",description:"dataset",source:"@site/docs/reference/sdk/dataset.md",sourceDirName:"reference/sdk",slug:"/reference/sdk/dataset",permalink:"/next/reference/sdk/dataset",draft:!1,editUrl:"https://github.com/star-whale/docs/tree/main/docs/reference/sdk/dataset.md",tags:[],version:"current",frontMatter:{title:"Starwhale Dataset SDK"},sidebar:"mainSidebar",previous:{title:"Python SDK Overview",permalink:"/next/reference/sdk/overview"},next:{title:"Starwhale Data Types",permalink:"/next/reference/sdk/type"}},s={},d=[{value:"dataset",id:"dataset",level:2},{value:"Parameters",id:"dataset-params",level:3},{value:"Examples",id:"dataset-example",level:3},{value:"class starwhale.Dataset",id:"class-starwhaledataset",level:2},{value:"from_huggingface",id:"from_huggingface",level:3},{value:"Parameters",id:"hf-params",level:4},{value:"Examples",id:"hf-example",level:4},{value:"from_json",id:"from_json",level:3},{value:"Parameters",id:"json-params",level:4},{value:"Examples",id:"json-example",level:4},{value:"from_folder",id:"from_folder",level:3},{value:"Parameters",id:"folder-params",level:4},{value:"Examples ${folder-example}",id:"examples-folder-example",level:4},{value:"__iter__",id:"__iter__",level:3},{value:"batch_iter",id:"batch_iter",level:3},{value:"Parameters",id:"batch-params",level:4},{value:"Examples",id:"batch-example",level:4},{value:"__getitem__",id:"__getitem__",level:3},{value:"__setitem__",id:"__setitem__",level:3},{value:"Parameters",id:"set-params",level:4},{value:"Examples",id:"set-example",level:4},{value:"__delitem__",id:"__delitem__",level:3},{value:"append",id:"append",level:3},{value:"extend",id:"extend",level:3},{value:"commit",id:"commit",level:3},{value:"Parameters",id:"commit-params",level:4},{value:"Examples",id:"commit-example",level:4},{value:"readonly",id:"readonly",level:3},{value:"loading_version",id:"loading_version",level:3},{value:"pending_commit_version",id:"pending_commit_version",level:3},{value:"committed_version",id:"committed_version",level:3},{value:"remove",id:"remove",level:3},{value:"recover",id:"recover",level:3},{value:"summary",id:"summary",level:3},{value:"history",id:"history",level:3},{value:"flush",id:"flush",level:3},{value:"close",id:"close",level:3},{value:"head",id:"head",level:3},{value:"fetch_one",id:"fetch_one",level:3},{value:"list",id:"list",level:3},{value:"copy",id:"copy",level:3},{value:"Parameters",id:"copy-params",level:4},{value:"Examples",id:"copy-example",level:4},{value:"to_pytorch",id:"to_pytorch",level:3},{value:"Parameters",id:"pytorch-params",level:4},{value:"Examples",id:"pytorch-example",level:4},{value:"to_tensorflow",id:"to_tensorflow",level:3},{value:"Parameters",id:"tf-params",level:4},{value:"Examples",id:"tf-example",level:4},{value:"with_builder_blob_config",id:"with_builder_blob_config",level:3},{value:"Parameters",id:"builder-params",level:4},{value:"Examples",id:"builder-example",level:4},{value:"with_loader_config",id:"with_loader_config",level:3},{value:"Parameters",id:"loader-params",level:4},{value:"Examples",id:"loader-example",level:4}],m={toc:d};function p(e){let{components:t,...a}=e;return(0,l.kt)("wrapper",(0,n.Z)({},m,a,{components:t,mdxType:"MDXLayout"}),(0,l.kt)("h2",{id:"dataset"},"dataset"),(0,l.kt)("p",null,"Get ",(0,l.kt)("inlineCode",{parentName:"p"},"starwhale.Dataset")," object, by creating new datasets or loading existing datasets."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"@classmethod\ndef dataset(\n    cls,\n    uri: t.Union[str, Resource],\n    create: str = _DatasetCreateMode.auto,\n    readonly: bool = False,\n) -> Dataset:\n")),(0,l.kt)("h3",{id:"dataset-params"},"Parameters"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"uri"),": (str or Resource, required)",(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"The dataset uri or Resource object."))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"create"),": (str, optional)",(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"The mode of dataset creating. The options are ",(0,l.kt)("inlineCode",{parentName:"li"},"auto"),", ",(0,l.kt)("inlineCode",{parentName:"li"},"empty")," and ",(0,l.kt)("inlineCode",{parentName:"li"},"forbid"),".",(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"auto")," mode: If the dataset already exists, creation is ignored. If it does not exist, the dataset is created automatically."),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"empty")," mode: If the dataset already exists, an Exception is raised; If it does not exist, an empty dataset is created. This mode ensures the creation of a new, empty dataset."),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"forbid")," mode: If the dataset already exists, nothing is done.If it does not exist, an Exception is raised. This mode ensures the existence of the dataset."))),(0,l.kt)("li",{parentName:"ul"},"The default is ",(0,l.kt)("inlineCode",{parentName:"li"},"auto"),"."))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"readonly"),": (bool, optional)",(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"For an existing dataset, you can specify the ",(0,l.kt)("inlineCode",{parentName:"li"},"readonly=True")," argument to ensure the dataset is in readonly mode."),(0,l.kt)("li",{parentName:"ul"},"Default is ",(0,l.kt)("inlineCode",{parentName:"li"},"False"),".")))),(0,l.kt)("h3",{id:"dataset-example"},"Examples"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'from starwhale import dataset, Image\n\n# create a new dataset named mnist, and add a row into the dataset\n# dataset("mnist") is equal to dataset("mnist", create="auto")\nds = dataset("mnist")\nds.exists()  # return False, "mnist" dataset is not existing.\nds.append({"img": Image(), "label": 1})\nds.commit()\nds.close()\n\n# load a cloud instance dataset in readonly mode\nds = dataset("cloud://remote-instance/project/starwhale/dataset/mnist", readonly=True)\nlabels = [row.features.label in ds]\nds.close()\n\n# load a read/write dataset with a specified version\nds = dataset("mnist/version/mrrdczdbmzsw")\nds[0].features.label = 1\nds.commit()\nds.close()\n\n# create an empty dataset\nds = dataset("mnist-empty", create="empty")\n\n# ensure the dataset existence\nds = dataset("mnist-existed", create="forbid")\n')),(0,l.kt)("h2",{id:"class-starwhaledataset"},"class starwhale.Dataset"),(0,l.kt)("p",null,(0,l.kt)("inlineCode",{parentName:"p"},"starwhale.Dataset")," implements the abstraction of a Starwhale dataset, and can operate on datasets in Standalone/Server/Cloud instances."),(0,l.kt)("h3",{id:"from_huggingface"},"from_huggingface"),(0,l.kt)("p",null,(0,l.kt)("inlineCode",{parentName:"p"},"from_huggingface")," is a classmethod that can convert a Huggingface dataset into a Starwhale dataset."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'def from_huggingface(\n    cls,\n    name: str,\n    repo: str,\n    subset: str | None = None,\n    split: str | None = None,\n    revision: str = "main",\n    alignment_size: int | str = D_ALIGNMENT_SIZE,\n    volume_size: int | str = D_FILE_VOLUME_SIZE,\n    mode: DatasetChangeMode | str = DatasetChangeMode.PATCH,\n    cache: bool = True,\n    tags: t.List[str] | None = None,\n) -> Dataset:\n')),(0,l.kt)("h4",{id:"hf-params"},"Parameters"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"name"),": (str, required)",(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"dataset name."))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"repo"),": (str, required)",(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"The huggingface datasets repo name, for example: ",(0,l.kt)("a",{parentName:"li",href:"https://huggingface.co/datasets/samsum"},"samsum"),", ",(0,l.kt)("a",{parentName:"li",href:"https://huggingface.co/datasets/fka/awesome-chatgpt-prompts"},"fka/awesome-chatgpt-prompts"),"\u3002"))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"subset"),": (str, optional)",(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"The subset name. If the huggingface dataset has multiple subsets, you must specify the subset name."))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"split"),": (str, optional)",(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"The split name. If the split name is not specified, the all splits dataset will be built."))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"revision"),": (str, optional)",(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"The huggingface datasets revision. The default value is ",(0,l.kt)("inlineCode",{parentName:"li"},"main"),". If the split name is not specified, the all splits dataset will be built."))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"alignment_size"),": (int|str, optional)",(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"The blob alignment size."),(0,l.kt)("li",{parentName:"ul"},"The default value is 128 Bytes."))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"volume_size"),": (int|str, optional)",(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"The maximum size of a dataset blob file. A new blob file will be generated when the size exceeds this limit."),(0,l.kt)("li",{parentName:"ul"},"The default value is ",(0,l.kt)("inlineCode",{parentName:"li"},"64MB"),"."))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"mode"),": (str|DatasetChangeMode, optional)",(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"The dataset change mode. The default value is ",(0,l.kt)("inlineCode",{parentName:"li"},"patch"),". Mode choices are ",(0,l.kt)("inlineCode",{parentName:"li"},"patch")," and ",(0,l.kt)("inlineCode",{parentName:"li"},"overwrite"),"."))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"cache"),": (bool, optional)",(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"Whether to use huggingface dataset cache(download + local hf dataset)."),(0,l.kt)("li",{parentName:"ul"},"The default value is True."))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"tags"),": (List","[str]",", optional)",(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"The user custom tags of the dataset.")))),(0,l.kt)("h4",{id:"hf-example"},"Examples"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'from starwhale import Dataset\nmyds = Dataset.from_huggingface("mnist", "mnist")\nprint(myds[0])\n')),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'from starwhale import Dataset\nmyds = Dataset.from_huggingface("mmlu", "cais/mmlu", subset="anatomy", split="auxiliary_train", revision="7456cfb")\n')),(0,l.kt)("h3",{id:"from_json"},"from_json"),(0,l.kt)("p",null,(0,l.kt)("inlineCode",{parentName:"p"},"from_json")," is a classmethod that can convert a json text into a Starwhale dataset."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'@classmethod\ndef from_json(\n    cls,\n    name: str,\n    json_text: str,\n    field_selector: str = "",\n    alignment_size: int | str = D_ALIGNMENT_SIZE,\n    volume_size: int | str = D_FILE_VOLUME_SIZE,\n    mode: DatasetChangeMode | str = DatasetChangeMode.PATCH,\n    tags: t.List[str] | None = None,\n) -> Dataset:\n')),(0,l.kt)("h4",{id:"json-params"},"Parameters"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"name"),": (str, required)",(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"Dataset name."))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"json_text"),": (str, required)",(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"A json string. The ",(0,l.kt)("inlineCode",{parentName:"li"},"from_json")," function deserializes this string into Python objects to start building the Starwhale dataset."))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"field_selector"),": (str, optional)",(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"The filed from which you would like to extract dataset array items."),(0,l.kt)("li",{parentName:"ul"},'The default value is "" which indicates that the json object is an array contains all the items.'))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"alignment_size"),": (int|str, optional)",(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"The blob alignment size."),(0,l.kt)("li",{parentName:"ul"},"The default value is 128 Bytes."))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"volume_size"),": (int|str, optional)",(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"The maximum size of a dataset blob file. A new blob file will be generated when the size exceeds this limit."),(0,l.kt)("li",{parentName:"ul"},"The default value is ",(0,l.kt)("inlineCode",{parentName:"li"},"64MB"),"."))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"mode"),": (str|DatasetChangeMode, optional)",(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"The dataset change mode. The default value is ",(0,l.kt)("inlineCode",{parentName:"li"},"patch"),". Mode choices are ",(0,l.kt)("inlineCode",{parentName:"li"},"patch")," and ",(0,l.kt)("inlineCode",{parentName:"li"},"overwrite"),"."))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"tags"),": (List","[str]",", optional)",(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"The user custom tags of the dataset.")))),(0,l.kt)("h4",{id:"json-example"},"Examples"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'from starwhale import Dataset\nmyds = Dataset.from_json(\n    name="translation",\n    json_text=\'[{"en":"hello","zh-cn":"\u4f60\u597d"},{"en":"how are you","zh-cn":"\u6700\u8fd1\u600e\u4e48\u6837"}]\'\n)\nprint(myds[0].features.en)\n')),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'from starwhale import Dataset\nmyds = Dataset.from_json(\n    name="translation",\n    json_text=\'{"content":{"child_content":[{"en":"hello","zh-cn":"\u4f60\u597d"},{"en":"how are you","zh-cn":"\u6700\u8fd1\u600e\u4e48\u6837"}]}}\',\n    field_selector="content.child_content"\n)\nprint(myds[0].features["zh-cn"])\n')),(0,l.kt)("h3",{id:"from_folder"},"from_folder"),(0,l.kt)("p",null,(0,l.kt)("inlineCode",{parentName:"p"},"from_folder")," is a classmethod that can read Image/Video/Audio data from a specified directory and automatically convert them into a Starwhale dataset. This function supports the following features:"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"It can recursively search the target directory and its subdirectories"),(0,l.kt)("li",{parentName:"ul"},"Supports extracting three types of files:",(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"image"),": Supports ",(0,l.kt)("inlineCode",{parentName:"li"},"png/jpg/jpeg/webp/svg/apng")," image types. Image files will be converted to ",(0,l.kt)("inlineCode",{parentName:"li"},"Starwhale.Image")," type."),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"video"),": Supports ",(0,l.kt)("inlineCode",{parentName:"li"},"mp4/webm/avi")," video types. Video files will be converted to ",(0,l.kt)("inlineCode",{parentName:"li"},"Starwhale.Video")," type."),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"audio"),": Supports ",(0,l.kt)("inlineCode",{parentName:"li"},"mp3/wav")," audio types. Audio files will be converted to ",(0,l.kt)("inlineCode",{parentName:"li"},"Starwhale.Audio")," type."))),(0,l.kt)("li",{parentName:"ul"},"Each file corresponds to one record in the dataset, with the file stored in the ",(0,l.kt)("inlineCode",{parentName:"li"},"file")," field."),(0,l.kt)("li",{parentName:"ul"},"If ",(0,l.kt)("inlineCode",{parentName:"li"},"auto_label=True"),", the parent directory name will be used as the label for that record, stored in the ",(0,l.kt)("inlineCode",{parentName:"li"},"label")," field. Files in the root directory will not be labeled."),(0,l.kt)("li",{parentName:"ul"},"If a txt file with the same name as an image/video/audio file exists, its content will be stored as the ",(0,l.kt)("inlineCode",{parentName:"li"},"caption")," field in the dataset."),(0,l.kt)("li",{parentName:"ul"},"If ",(0,l.kt)("inlineCode",{parentName:"li"},"metadata.csv")," or ",(0,l.kt)("inlineCode",{parentName:"li"},"metadata.jsonl")," exists in the root directory, their content will be read automatically and associated to records by file path as meta information in the dataset.",(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"metadata.csv")," and ",(0,l.kt)("inlineCode",{parentName:"li"},"metadata.jsonl")," are mutually exclusive. An exception will be thrown if both exist."),(0,l.kt)("li",{parentName:"ul"},"Each record in ",(0,l.kt)("inlineCode",{parentName:"li"},"metadata.csv")," and ",(0,l.kt)("inlineCode",{parentName:"li"},"metadata.jsonl")," must contain a ",(0,l.kt)("inlineCode",{parentName:"li"},"file_name")," field pointing to the file path."),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"metadata.csv")," and ",(0,l.kt)("inlineCode",{parentName:"li"},"metadata.jsonl")," are optional for dataset building.")))),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'@classmethod\ndef from_folder(\n    cls,\n    folder: str | Path,\n    kind: str | DatasetFolderSourceType,\n    name: str | Resource = "",\n    auto_label: bool = True,\n    alignment_size: int | str = D_ALIGNMENT_SIZE,\n    volume_size: int | str = D_FILE_VOLUME_SIZE,\n    mode: DatasetChangeMode | str = DatasetChangeMode.PATCH,\n    tags: t.List[str] | None = None,\n) -> Dataset:\n')),(0,l.kt)("h4",{id:"folder-params"},"Parameters"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"folder"),": (str|Path, required)",(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"The folder path from which you would like to create this dataset."))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"kind"),": (str|DatasetFolderSourceType, required)",(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"The dataset source type you would like to use, the choices are: image, video and audio."),(0,l.kt)("li",{parentName:"ul"},"Recursively searching for files of the specified ",(0,l.kt)("inlineCode",{parentName:"li"},"kind")," in ",(0,l.kt)("inlineCode",{parentName:"li"},"folder"),". Other file types will be ignored."))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"name"),": (str|Resource, optional)",(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"The dataset name you would like to use."),(0,l.kt)("li",{parentName:"ul"},"If not specified, the name is the folder name."))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"auto_label"),": (bool, optional)",(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"Whether to auto label by the sub-folder name."),(0,l.kt)("li",{parentName:"ul"},"The default value is ",(0,l.kt)("inlineCode",{parentName:"li"},"True"),"."))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"alignment_size"),": (int|str, optional)",(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"The blob alignment size."),(0,l.kt)("li",{parentName:"ul"},"The default value is 128 Bytes."))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"volume_size"),": (int|str, optional)",(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"The maximum size of a dataset blob file. A new blob file will be generated when the size exceeds this limit."),(0,l.kt)("li",{parentName:"ul"},"The default value is ",(0,l.kt)("inlineCode",{parentName:"li"},"64MB"),"."))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"mode"),": (str|DatasetChangeMode, optional)",(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"The dataset change mode. The default value is ",(0,l.kt)("inlineCode",{parentName:"li"},"patch"),". Mode choices are ",(0,l.kt)("inlineCode",{parentName:"li"},"patch")," and ",(0,l.kt)("inlineCode",{parentName:"li"},"overwrite"),"."))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"tags"),": (List","[str]",", optional)",(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"The user custom tags of the dataset.")))),(0,l.kt)("h4",{id:"examples-folder-example"},"Examples ${folder-example}"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("p",{parentName:"li"},"Example for the normal function calling"),(0,l.kt)("pre",{parentName:"li"},(0,l.kt)("code",{parentName:"pre",className:"language-python"},'from starwhale import Dataset\n\n# create a my-image-dataset dataset from /path/to/image folder.\nds = Dataset.from_folder(\n    folder="/path/to/image",\n    kind="image",\n    name="my-image-dataset"\n)\n'))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("p",{parentName:"li"},"Example for ",(0,l.kt)("inlineCode",{parentName:"p"},"caption")),(0,l.kt)("pre",{parentName:"li"},(0,l.kt)("code",{parentName:"pre",className:"language-bash"},"folder/dog/1.png\nfolder/dog/1.txt\n")),(0,l.kt)("p",{parentName:"li"},"1.txt content will be used as the caption of 1.png.")),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("p",{parentName:"li"},"Example for ",(0,l.kt)("inlineCode",{parentName:"p"},"metadata")),(0,l.kt)("p",{parentName:"li"},(0,l.kt)("inlineCode",{parentName:"p"},"metadata.csv"),":"),(0,l.kt)("pre",{parentName:"li"},(0,l.kt)("code",{parentName:"pre",className:"language-csv"},"file_name, caption\n1.png, dog\n2.png, cat\n")),(0,l.kt)("p",{parentName:"li"},(0,l.kt)("inlineCode",{parentName:"p"},"metadata.jsonl"),":"),(0,l.kt)("pre",{parentName:"li"},(0,l.kt)("code",{parentName:"pre",className:"language-jsonl"},'{"file_name": "1.png", "caption": "dog"}\n{"file_name": "2.png", "caption": "cat"}\n'))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("p",{parentName:"li"},"Example for auto-labeling"),(0,l.kt)("p",{parentName:"li"},'The following structure will create a dataset with 2 labels: "cat" and "dog", 4 images in total.'),(0,l.kt)("pre",{parentName:"li"},(0,l.kt)("code",{parentName:"pre",className:"language-bash"},"folder/dog/1.png\nfolder/cat/2.png\nfolder/dog/3.png\nfolder/cat/4.png\n")))),(0,l.kt)("h3",{id:"__iter__"},"_","_","iter","_","_"),(0,l.kt)("p",null,(0,l.kt)("inlineCode",{parentName:"p"},"__iter__")," a method that iter the dataset rows."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'from starwhale import dataset\n\nds = dataset("mnist")\n\nfor item in ds:\n  print(item.index)\n  print(item.features.label)  # label and img are the features of mnist.\n  print(item.features.img)\n')),(0,l.kt)("h3",{id:"batch_iter"},"batch_iter"),(0,l.kt)("p",null,(0,l.kt)("inlineCode",{parentName:"p"},"batch_iter")," is a method that iter the dataset rows in batch."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"def batch_iter(\n    self, batch_size: int = 1, drop_not_full: bool = False\n) -> t.Iterator[t.List[DataRow]]:\n")),(0,l.kt)("h4",{id:"batch-params"},"Parameters"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"batch_size"),": (int, optional)",(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"batch size. The default value is 1."))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"drop_not_full"),": (bool, optional)",(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"Whether the last batch of data, with a size smaller than ",(0,l.kt)("inlineCode",{parentName:"li"},"batch_size"),", it will be discarded."),(0,l.kt)("li",{parentName:"ul"},"The default value is ",(0,l.kt)("inlineCode",{parentName:"li"},"False"),".")))),(0,l.kt)("h4",{id:"batch-example"},"Examples"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'from starwhale import dataset\n\nds = dataset("mnist")\nfor batch_rows in ds.batch_iter(batch_size=2):\n    assert len(batch_rows) == 2\n    print(batch_rows[0].features)\n')),(0,l.kt)("h3",{id:"__getitem__"},"_","_","getitem","_","_"),(0,l.kt)("p",null,(0,l.kt)("inlineCode",{parentName:"p"},"__getitem__")," is a method that allows retrieving certain rows of data from the dataset, with usage similar to Python dict and list types."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'from starwhale import dataset\n\nds = dataset("mock-int-index")\n\n# if the index type is string\nds["str_key"]  # get the DataRow by the "str_key" string key\nds["start":"end"]  # get a slice of the dataset by the range ("start", "end")\n\nds = dataset("mock-str-index")\n# if the index type is int\nds[1]  # get the DataRow by the 1 int key\nds[1:10:2] # get a slice of the dataset by the range (1, 10), step is 2\n')),(0,l.kt)("h3",{id:"__setitem__"},"_","_","setitem","_","_"),(0,l.kt)("p",null,(0,l.kt)("inlineCode",{parentName:"p"},"__setitem__")," is a method that allows updating rows of data in the dataset, with usage similar to Python dicts. ",(0,l.kt)("inlineCode",{parentName:"p"},"__setitem__")," supports multi-threaded parallel data insertion."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"def __setitem__(\n    self, key: t.Union[str, int], value: t.Union[DataRow, t.Tuple, t.Dict]\n) -> None:\n")),(0,l.kt)("h4",{id:"set-params"},"Parameters"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"key"),": (int|str, required)",(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"key")," is the ",(0,l.kt)("inlineCode",{parentName:"li"},"index")," for each row in the dataset. The type is int or str, but a dataset only accepts one type."))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"value"),": (DataRow|tuple|dict, required)",(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"value")," is the ",(0,l.kt)("inlineCode",{parentName:"li"},"features")," for each row in the dataset, using a Python dict is generally recommended.")))),(0,l.kt)("h4",{id:"set-example"},"Examples"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"Normal insertion")),(0,l.kt)("p",null,"Insert two rows into the ",(0,l.kt)("inlineCode",{parentName:"p"},"test")," dataset, with index ",(0,l.kt)("inlineCode",{parentName:"p"},"test")," and ",(0,l.kt)("inlineCode",{parentName:"p"},"test2")," repectively:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'from starwhale import dataset\n\nwith dataset("test") as ds:\n  ds["test"] = {"txt": "abc", "int": 1}\n  ds["test2"] = {"txt": "bcd", "int": 2}\n  ds.commit()\n')),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"Parallel insertion")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'from starwhale import dataset, Binary\nfrom concurrent.futures import as_completed, ThreadPoolExecutor\n\nds = dataset("test")\n\ndef _do_append(_start: int) -> None:\n  for i in range(_start, 100):\n    ds.append((i, {"data": Binary(), "label": i}))\n\npool = ThreadPoolExecutor(max_workers=10)\ntasks = [pool.submit(_do_append, i * 10) for i in range(0, 9)]\n\nds.commit()\nds.close()\n')),(0,l.kt)("h3",{id:"__delitem__"},"_","_","delitem","_","_"),(0,l.kt)("p",null,(0,l.kt)("inlineCode",{parentName:"p"},"__delitem__")," is a method to delete certain rows of data from the dataset."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"def __delitem__(self, key: _ItemType) -> None:\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'from starwhale import dataset\n\nds = dataset("existed-ds")\ndel ds[6:9]\ndel ds[0]\nds.commit()\nds.close()\n')),(0,l.kt)("h3",{id:"append"},"append"),(0,l.kt)("p",null,(0,l.kt)("inlineCode",{parentName:"p"},"append")," is a method to append data to a dataset, similar to the append method for Python lists."),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("p",{parentName:"li"},"Adding features dict, each row is automatically indexed with int starting from 0 and incrementing."),(0,l.kt)("pre",{parentName:"li"},(0,l.kt)("code",{parentName:"pre",className:"language-python"},'from starwhale import dataset, Image\n\nwith dataset("new-ds") as ds:\n  for i in range(0, 100):\n    ds.append({"label": i, "image": Image(f"folder/{i}.png")})\n  ds.commit()\n'))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("p",{parentName:"li"},"By appending the index and features dictionary, the index of each data row in the dataset will not be handled automatically."),(0,l.kt)("pre",{parentName:"li"},(0,l.kt)("code",{parentName:"pre",className:"language-python"},'from dataset import dataset, Image\n\nwith dataset("new-ds") as ds:\n  for i in range(0, 100):\n    ds.append((f"index-{i}", {"label": i, "image": Image(f"folder/{i}.png")}))\n\n  ds.commit()\n')))),(0,l.kt)("h3",{id:"extend"},"extend"),(0,l.kt)("p",null,(0,l.kt)("inlineCode",{parentName:"p"},"extend")," is a method to bulk append data to a dataset, similar to the extend method for Python lists."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'from starwhale import dataset, Text\n\nds = dataset("new-ds")\nds.extend([\n    (f"label-{i}", {"text": Text(), "label": i}) for i in range(0, 10)\n  ])\nds.commit()\nds.close()\n')),(0,l.kt)("h3",{id:"commit"},"commit"),(0,l.kt)("p",null,(0,l.kt)("inlineCode",{parentName:"p"},"commit")," is a method that flushes the current cached data to storage when called, and generates a dataset version. This version can then be used to load the corresponding dataset content afterwards."),(0,l.kt)("p",null,"For a dataset, if some data is added without calling ",(0,l.kt)("inlineCode",{parentName:"p"},"commit"),", but ",(0,l.kt)("inlineCode",{parentName:"p"},"close")," is called or the process exits directly instead, the data will still be written to the dataset, just without generating a new version."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'@_check_readonly\ndef commit(\n    self,\n    tags: t.Optional[t.List[str]] = None,\n    message: str = "",\n    force_add_tags: bool = False,\n    ignore_add_tags_errors: bool = False,\n) -> str:\n')),(0,l.kt)("h4",{id:"commit-params"},"Parameters"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"tags"),": (list(str), optional)",(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"tag as a list"))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"message"),": (str, optional)",(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"commit message. The default value is empty."))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"force_add_tags"),": (bool, optional)",(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"For server/cloud instances, when adding labels to this version, if a label has already been applied to other dataset versions, you can use the ",(0,l.kt)("inlineCode",{parentName:"li"},"force_add_tags=True")," parameter to forcibly add the label to this version, otherwise an exception will be thrown."),(0,l.kt)("li",{parentName:"ul"},"The default is ",(0,l.kt)("inlineCode",{parentName:"li"},"False"),"."))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"ignore_add_tags_errors"),": (bool, optional)",(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"Ignore any exceptions thrown when adding labels."),(0,l.kt)("li",{parentName:"ul"},"The default is ",(0,l.kt)("inlineCode",{parentName:"li"},"False"),".")))),(0,l.kt)("h4",{id:"commit-example"},"Examples"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'from starwhale import dataset\nwith dataset("mnist") as ds:\n    ds.append({"label": 1})\n    ds.commit(message="init commit")\n')),(0,l.kt)("h3",{id:"readonly"},"readonly"),(0,l.kt)("p",null,(0,l.kt)("inlineCode",{parentName:"p"},"readonly")," is a property attribute indicating if the dataset is read-only, it returns a bool value."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'from starwhale import dataset\nds = dataset("mnist", readonly=True)\nassert ds.readonly\n')),(0,l.kt)("h3",{id:"loading_version"},"loading_version"),(0,l.kt)("p",null,(0,l.kt)("inlineCode",{parentName:"p"},"loading_version")," is a property attribute, string type."),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"When loading an existing dataset, the ",(0,l.kt)("inlineCode",{parentName:"li"},"loading_version")," is the related dataset version."),(0,l.kt)("li",{parentName:"ul"},"When creating a non-existed dataset, the ",(0,l.kt)("inlineCode",{parentName:"li"},"loading_version")," is equal to the pending_commit_version.")),(0,l.kt)("h3",{id:"pending_commit_version"},"pending_commit_version"),(0,l.kt)("p",null,(0,l.kt)("inlineCode",{parentName:"p"},"pending_commit_version")," is a property attribute, string type.  When you call the ",(0,l.kt)("inlineCode",{parentName:"p"},"commit")," function, the ",(0,l.kt)("inlineCode",{parentName:"p"},"pending_commit_version")," will be recorded in the Standalone instance ,Server instance or Cloud instance."),(0,l.kt)("h3",{id:"committed_version"},"committed_version"),(0,l.kt)("p",null,(0,l.kt)("inlineCode",{parentName:"p"},"committed_version")," is a property attribute, string type. After the ",(0,l.kt)("inlineCode",{parentName:"p"},"commit")," function is called, the ",(0,l.kt)("inlineCode",{parentName:"p"},"committed_version")," will come out, it is equal to the ",(0,l.kt)("inlineCode",{parentName:"p"},"pending_commit_version"),". Accessing this attribute without calling ",(0,l.kt)("inlineCode",{parentName:"p"},"commit")," first will raise an exception."),(0,l.kt)("h3",{id:"remove"},"remove"),(0,l.kt)("p",null,(0,l.kt)("inlineCode",{parentName:"p"},"remove")," is a method equivalent to the ",(0,l.kt)("inlineCode",{parentName:"p"},"swcli dataset remove")," command, it can delete a dataset."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"def remove(self, force: bool = False) -> None:\n")),(0,l.kt)("h3",{id:"recover"},"recover"),(0,l.kt)("p",null,(0,l.kt)("inlineCode",{parentName:"p"},"recover")," is a method equivalent to the ",(0,l.kt)("inlineCode",{parentName:"p"},"swcli dataset recover")," command, it can recover a soft-deleted dataset that has not been run garbage collection."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"def recover(self, force: bool = False) -> None:\n")),(0,l.kt)("h3",{id:"summary"},"summary"),(0,l.kt)("p",null,(0,l.kt)("inlineCode",{parentName:"p"},"summary")," is a method equivalent to the ",(0,l.kt)("inlineCode",{parentName:"p"},"swcli dataset summary")," command, it returns summary information of the dataset."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"def summary(self) -> t.Optional[DatasetSummary]:\n")),(0,l.kt)("h3",{id:"history"},"history"),(0,l.kt)("p",null,(0,l.kt)("inlineCode",{parentName:"p"},"history")," is a method equivalent to the ",(0,l.kt)("inlineCode",{parentName:"p"},"swcli dataset history")," command, it returns the history records of the dataset."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"def history(self) -> t.List[t.Dict]:\n")),(0,l.kt)("h3",{id:"flush"},"flush"),(0,l.kt)("p",null,(0,l.kt)("inlineCode",{parentName:"p"},"flush")," is a method that flushes temporarily cached data from memory to persistent storage. The ",(0,l.kt)("inlineCode",{parentName:"p"},"commit")," and ",(0,l.kt)("inlineCode",{parentName:"p"},"close")," methods will automatically call ",(0,l.kt)("inlineCode",{parentName:"p"},"flush"),"."),(0,l.kt)("h3",{id:"close"},"close"),(0,l.kt)("p",null,(0,l.kt)("inlineCode",{parentName:"p"},"close")," is a method that closes opened connections related to the dataset. ",(0,l.kt)("inlineCode",{parentName:"p"},"Dataset")," also implements contextmanager, so datasets can be automatically closed using ",(0,l.kt)("inlineCode",{parentName:"p"},"with")," syntax without needing to explicitly call ",(0,l.kt)("inlineCode",{parentName:"p"},"close"),"."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'from starwhale import dataset\n\nds = dataset("mnist")\nds.close()\n\nwith dataset("mnist") as ds:\n  print(ds[0])\n')),(0,l.kt)("h3",{id:"head"},"head"),(0,l.kt)("p",null,(0,l.kt)("inlineCode",{parentName:"p"},"head")," is a method to show the first n rows of a dataset, equivalent to the ",(0,l.kt)("inlineCode",{parentName:"p"},"swcli dataset head")," command."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"def head(self, n: int = 5, skip_fetch_data: bool = False) -> List[DataRow]:\n")),(0,l.kt)("h3",{id:"fetch_one"},"fetch_one"),(0,l.kt)("p",null,(0,l.kt)("inlineCode",{parentName:"p"},"fetch_one")," is a method to get the first record in a dataset, similar to ",(0,l.kt)("inlineCode",{parentName:"p"},"head(n=1)[0]"),"."),(0,l.kt)("h3",{id:"list"},"list"),(0,l.kt)("p",null,(0,l.kt)("inlineCode",{parentName:"p"},"list")," is a class method to list Starwhale datasets under a project URI, equivalent to the ",(0,l.kt)("inlineCode",{parentName:"p"},"swcli dataset list")," command."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'@classmethod\ndef list(\n    cls,\n    project_uri: Union[str, Project] = "",\n    fullname: bool = False,\n    show_removed: bool = False,\n    page_index: int = DEFAULT_PAGE_IDX,\n    page_size: int = DEFAULT_PAGE_SIZE,\n) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:\n')),(0,l.kt)("h3",{id:"copy"},"copy"),(0,l.kt)("p",null,(0,l.kt)("inlineCode",{parentName:"p"},"copy")," is a method to copy a dataset to another instance, equivalent to the ",(0,l.kt)("inlineCode",{parentName:"p"},"swcli dataset copy")," command."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'def copy(\n  self,\n  dest_uri: str,\n  dest_local_project_uri: str = "",\n  force: bool = False,\n  mode: str = DatasetChangeMode.PATCH.value,\n  ignore_tags: t.List[str] | None = None,\n) -> None:\n')),(0,l.kt)("h4",{id:"copy-params"},"Parameters"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"dest_uri"),": (str, required)",(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"Dataset URI"))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"dest_local_project_uri"),": (str, optional)",(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"When copy the remote dataset into local, the parameter can set for the Project URI."))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"force"),": (bool, optional)",(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"Whether to forcibly overwrite the dataset if there is already one with the same version on the target instance."),(0,l.kt)("li",{parentName:"ul"},"The default value is ",(0,l.kt)("inlineCode",{parentName:"li"},"False"),"."),(0,l.kt)("li",{parentName:"ul"},"When the tags are already used for the other dataset version in the dest instance, you should use ",(0,l.kt)("inlineCode",{parentName:"li"},"force")," option or adjust the tags."))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"mode"),": (str, optional)",(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"Dataset copy mode, default is 'patch'. Mode choices are: 'patch', 'overwrite'."),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"patch"),": Patch mode, only update the changed rows and columns for the remote dataset."),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"overwrite"),": Overwrite mode, update records and delete extraneous rows from the remote dataset."))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"ignore_tags")," (List","[str]",", optional)",(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"Ignore tags when copying."),(0,l.kt)("li",{parentName:"ul"},"In default, copy dataset with all user custom tags."),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"latest")," and ",(0,l.kt)("inlineCode",{parentName:"li"},"^v\\d+$")," are the system builtin tags, they are ignored automatically.")))),(0,l.kt)("h4",{id:"copy-example"},"Examples"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'from starwhale import dataset\nds = dataset("mnist")\nds.copy("cloud://remote-instance/project/starwhale")\n')),(0,l.kt)("h3",{id:"to_pytorch"},"to_pytorch"),(0,l.kt)("p",null,(0,l.kt)("inlineCode",{parentName:"p"},"to_pytorch")," is a method that can convert a Starwhale dataset to a Pytorch ",(0,l.kt)("inlineCode",{parentName:"p"},"torch.utils.data.Dataset"),", which can then be passed to ",(0,l.kt)("inlineCode",{parentName:"p"},"torch.utils.data.DataLoader")," for use."),(0,l.kt)("p",null,(0,l.kt)("strong",{parentName:"p"},"It should be noted that the ",(0,l.kt)("inlineCode",{parentName:"strong"},"to_pytorch")," function returns a Pytorch ",(0,l.kt)("inlineCode",{parentName:"strong"},"IterableDataset"),".")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"def to_pytorch(\n    self,\n    transform: t.Optional[t.Callable] = None,\n    drop_index: bool = True,\n    skip_default_transform: bool = False,\n) -> torch.utils.data.Dataset:\n")),(0,l.kt)("h4",{id:"pytorch-params"},"Parameters"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"transform"),": (callable, optional)",(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"A transform function for input data."))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"drop_index"),": (bool, optional)",(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"Whether to drop the ",(0,l.kt)("inlineCode",{parentName:"li"},"index")," column."))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"skip_default_transform"),": (bool, optional)",(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"If ",(0,l.kt)("inlineCode",{parentName:"li"},"transform")," is not set, by default the built-in Starwhale transform function will be used to transform the data. This can be disabled with the ",(0,l.kt)("inlineCode",{parentName:"li"},"skip_default_transform")," parameter.")))),(0,l.kt)("h4",{id:"pytorch-example"},"Examples"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'import torch.utils.data as tdata\nfrom starwhale import dataset\n\nds = dataset("mnist")\n\ntorch_ds = ds.to_pytorch()\ntorch_loader = tdata.DataLoader(torch_ds, batch_size=2)\n')),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'import torch.utils.data as tdata\nfrom starwhale import dataset\n\nwith dataset("mnist") as ds:\n    for i in range(0, 10):\n        ds.append({"txt": Text(f"data-{i}"), "label": i})\n\n    ds.commit()\n\ndef _custom_transform(data: t.Any) -> t.Any:\n    data = data.copy()\n    txt = data["txt"].to_str()\n    data["txt"] = f"custom-{txt}"\n    return data\n\ntorch_loader = tdata.DataLoader(\n    dataset(ds.uri).to_pytorch(transform=_custom_transform), batch_size=1\n)\nitem = next(iter(torch_loader))\nassert isinstance(item["label"], torch.Tensor)\nassert item["txt"][0] in ("custom-data-0", "custom-data-1")\n')),(0,l.kt)("h3",{id:"to_tensorflow"},"to_tensorflow"),(0,l.kt)("p",null,(0,l.kt)("inlineCode",{parentName:"p"},"to_tensorflow")," is a method that can convert a Starwhale dataset to a Tensorflow ",(0,l.kt)("inlineCode",{parentName:"p"},"tensorflow.data.Dataset"),"."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"def to_tensorflow(self, drop_index: bool = True) -> tensorflow.data.Dataset:\n")),(0,l.kt)("h4",{id:"tf-params"},"Parameters"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"drop_index"),": (bool, optional)",(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"Whether to drop the ",(0,l.kt)("inlineCode",{parentName:"li"},"index")," column.")))),(0,l.kt)("h4",{id:"tf-example"},"Examples"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'from starwhale import dataset\nimport tensorflow as tf\n\nds = dataset("mnist")\ntf_ds = ds.to_tensorflow(drop_index=True)\nassert isinstance(tf_ds, tf.data.Dataset)\n')),(0,l.kt)("h3",{id:"with_builder_blob_config"},"with_builder_blob_config"),(0,l.kt)("p",null,(0,l.kt)("inlineCode",{parentName:"p"},"with_builder_blob_config")," is a method to set blob-related attributes in a Starwhale dataset. It needs to be called before making data changes."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"def with_builder_blob_config(\n    self,\n    volume_size: int | str | None = D_FILE_VOLUME_SIZE,\n    alignment_size: int | str | None = D_ALIGNMENT_SIZE,\n) -> Dataset:\n")),(0,l.kt)("h4",{id:"builder-params"},"Parameters"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"alignment_size"),": (int|str, optional)",(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"The blob alignment size."),(0,l.kt)("li",{parentName:"ul"},"The default value is 128 Bytes."))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"volume_size"),": (int|str, optional)",(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"The maximum size of a dataset blob file. A new blob file will be generated when the size exceeds this limit."),(0,l.kt)("li",{parentName:"ul"},"The default value is ",(0,l.kt)("inlineCode",{parentName:"li"},"64MB"),".")))),(0,l.kt)("h4",{id:"builder-example"},"Examples"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'from starwhale import dataset, Binary\n\nds = dataset("mnist").with_builder_blob_config(volume_size="32M", alignment_size=128)\nds.append({"data": Binary(b"123")})\nds.commit()\nds.close()\n')),(0,l.kt)("h3",{id:"with_loader_config"},"with_loader_config"),(0,l.kt)("p",null,(0,l.kt)("inlineCode",{parentName:"p"},"with_loader_config")," is a method to set parameters for the Starwhale dataset loader process."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"def with_loader_config(\n    self,\n    num_workers: t.Optional[int] = None,\n    cache_size: t.Optional[int] = None,\n    field_transformer: t.Optional[t.Dict] = None,\n) -> Dataset:\n")),(0,l.kt)("h4",{id:"loader-params"},"Parameters"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"num_workers"),": (int, optional)",(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"The workers number for loading dataset."),(0,l.kt)("li",{parentName:"ul"},"The default value is 2."))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"cache_size"),": (int, optional)",(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"Prefetched data rows."),(0,l.kt)("li",{parentName:"ul"},"The default value is 20."))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"field_transformer"),": (dict, optional)",(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"features name transform dict.")))),(0,l.kt)("h4",{id:"loader-example"},"Examples"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'from starwhale import Dataset, dataset\nDataset.from_json(\n    "translation",\n    \'[{"en":"hello","zh-cn":"\u4f60\u597d"},{"en":"how are you","zh-cn":"\u6700\u8fd1\u600e\u4e48\u6837"}]\'\n)\nmyds = dataset("translation").with_loader_config(field_transformer={"en": "en-us"})\nassert myds[0].features["en-us"] == myds[0].features["en"]\n')),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'from starwhale import Dataset, dataset\nDataset.from_json(\n    "translation2",\n    \'[{"content":{"child_content":[{"en":"hello","zh-cn":"\u4f60\u597d"},{"en":"how are you","zh-cn":"\u6700\u8fd1\u600e\u4e48\u6837"}]}}]\'\n)\nmyds = dataset("translation2").with_loader_config(field_transformer={"content.child_content[0].en": "en-us"})\nassert myds[0].features["en-us"] == myds[0].features["content"]["child_content"][0]["en"]\n')))}p.isMDXComponent=!0}}]);