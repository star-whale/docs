"use strict";(self.webpackChunkstarwhale_docs=self.webpackChunkstarwhale_docs||[]).push([[56882],{3905:(e,t,a)=>{a.d(t,{Zo:()=>m,kt:()=>g});var n=a(67294);function r(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function i(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function o(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?i(Object(a),!0).forEach((function(t){r(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):i(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function l(e,t){if(null==e)return{};var a,n,r=function(e,t){if(null==e)return{};var a,n,r={},i=Object.keys(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var s=n.createContext({}),p=function(e){var t=n.useContext(s),a=t;return e&&(a="function"==typeof e?e(t):o(o({},t),e)),a},m=function(e){var t=p(e.components);return n.createElement(s.Provider,{value:t},e.children)},c={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},d=n.forwardRef((function(e,t){var a=e.components,r=e.mdxType,i=e.originalType,s=e.parentName,m=l(e,["components","mdxType","originalType","parentName"]),d=p(a),g=r,u=d["".concat(s,".").concat(g)]||d[g]||c[g]||i;return a?n.createElement(u,o(o({ref:t},m),{},{components:a})):n.createElement(u,o({ref:t},m))}));function g(e,t){var a=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var i=a.length,o=new Array(i);o[0]=d;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l.mdxType="string"==typeof e?e:r,o[1]=l;for(var p=2;p<i;p++)o[p]=a[p];return n.createElement.apply(null,o)}return n.createElement.apply(null,a)}d.displayName="MDXCreateElement"},68062:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>s,contentTitle:()=>o,default:()=>c,frontMatter:()=>i,metadata:()=>l,toc:()=>p});var n=a(83117),r=(a(67294),a(3905));const i={title:"Integration with Other ML Libraries"},o=void 0,l={unversionedId:"dataset/integration",id:"dataset/integration",title:"Integration with Other ML Libraries",description:"Starwhale datasets can integrate well with popular ML libraries such as Pillow, Numpy, Huggingface Datasets, Pytorch and Tensorflow, facilitating data transformation.",source:"@site/docs/dataset/integration.md",sourceDirName:"dataset",slug:"/dataset/integration",permalink:"/next/dataset/integration",draft:!1,editUrl:"https://github.com/star-whale/docs/tree/main/docs/dataset/integration.md",tags:[],version:"current",frontMatter:{title:"Integration with Other ML Libraries"},sidebar:"mainSidebar",previous:{title:"Dataset Versioning",permalink:"/next/dataset/version"},next:{title:"Starwhale Model Evaluation",permalink:"/next/evaluation/"}},s={},p=[{value:"Pillow",id:"pillow",level:2},{value:"Initialize Starwhale Image with Pillow Image",id:"initialize-starwhale-image-with-pillow-image",level:3},{value:"Converting Starwhale Image to Pillow Image",id:"converting-starwhale-image-to-pillow-image",level:3},{value:"Numpy",id:"numpy",level:2},{value:"Converting to numpy.ndarray",id:"converting-to-numpyndarray",level:3},{value:"Initialize Starwhale Image with numpy.ndarray",id:"initialize-starwhale-image-with-numpyndarray",level:3},{value:"Huggingface Datasets",id:"huggingface-datasets",level:2},{value:"Pytorch",id:"pytorch",level:2},{value:"Tensorflow",id:"tensorflow",level:2}],m={toc:p};function c(e){let{components:t,...a}=e;return(0,r.kt)("wrapper",(0,n.Z)({},m,a,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("p",null,"Starwhale datasets can integrate well with popular ML libraries such as Pillow, Numpy, Huggingface Datasets, Pytorch and Tensorflow, facilitating data transformation."),(0,r.kt)("h2",{id:"pillow"},"Pillow"),(0,r.kt)("p",null,(0,r.kt)("a",{parentName:"p",href:"../reference/sdk/type#image"},"Starwhale Image")," type and ",(0,r.kt)("a",{parentName:"p",href:"https://pillow.readthedocs.io/en/stable/reference/Image.html"},"Pillow Image")," objects have bidirectional conversion."),(0,r.kt)("h3",{id:"initialize-starwhale-image-with-pillow-image"},"Initialize Starwhale Image with Pillow Image"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'from starwhale import dataset\n\n# login cloud instance in advance: `swcli instance login` command or `starwhale.login` sdk\n# raw dataset url: https://cloud.starwhale.cn/projects/397/datasets/172/versions/236/files\nds = dataset("https://cloud.starwhale.cn/project/starwhale:object-detection/dataset/coco128/v2")\nimg = ds.head(n=1)[0].features.image\n\npil = img.to_pil()\nprint(pil)\nprint(pil.size)\n')),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-console"},"<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x480 at 0x7F77FBA98250>\n(640, 480)\n")),(0,r.kt)("h3",{id:"converting-starwhale-image-to-pillow-image"},"Converting Starwhale Image to Pillow Image"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'import numpy\nfrom PIL import Image as PILImage\nfrom starwhale import Image\n\n# generate a random image\nrandom_array = numpy.random.randint(low=0, high=256, size=(100, 100, 3), dtype=numpy.uint8)\npil = PILImage.fromarray(random_array, mode="RGB")\n\nimg = Image(pil)\nprint(img)\n')),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-console"},"ArtifactType.Image, display:, mime_type:MIMEType.UNDEFINED, shape:[None, None, 3], encoding:\n")),(0,r.kt)("h2",{id:"numpy"},"Numpy"),(0,r.kt)("h3",{id:"converting-to-numpyndarray"},"Converting to numpy.ndarray"),(0,r.kt)("p",null,"The following Starwhale data types can be converted to ",(0,r.kt)("inlineCode",{parentName:"p"},"numpy.ndarray")," objects:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"Image"),": First convert to Pillow Image type, then to ",(0,r.kt)("inlineCode",{parentName:"li"},"numpy.ndarray")," object."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"Video"),": Directly convert video bytes to ",(0,r.kt)("inlineCode",{parentName:"li"},"numpy.ndarray")," object."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"Audio"),": Use the soundfile library to convert audio bytes to ",(0,r.kt)("inlineCode",{parentName:"li"},"numpy.ndarray")," object."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"BoundingBox"),": Convert to ",(0,r.kt)("inlineCode",{parentName:"li"},"numpy.ndarray")," object in xywh format."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"Binary"),": Directly convert bytes to ",(0,r.kt)("inlineCode",{parentName:"li"},"numpy.ndarray")," object.")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'from starwhale import dataset\n\n# login cloud instance in advance: `swcli instance login` command or `starwhale.login` sdk\n# raw dataset url: https://cloud.starwhale.cn/projects/397/datasets/172/versions/236/files\nds = dataset("https://cloud.starwhale.cn/project/starwhale:object-detection/dataset/coco128/v2")\n\nitem = ds.head(n=1)[0]\n\nimg = item.features.image\nimg_array = img.to_numpy()\nprint(img_array)\nprint(img_array.shape)\n\nbbox = item.features.annotations[0]["bbox"]\nprint(bbox)\nprint(bbox.to_numpy())\n')),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-console"},"<class 'numpy.ndarray'>\n(480, 640, 3)\nBoundingBox[XYWH]- x:1.0799999999999699, y:187.69008, width:611.5897600000001, height:285.84000000000003\narray([  1.08   , 187.69008, 611.58976, 285.84   ])\n")),(0,r.kt)("h3",{id:"initialize-starwhale-image-with-numpyndarray"},"Initialize Starwhale Image with numpy.ndarray"),(0,r.kt)("p",null,"When an image is represented as a ",(0,r.kt)("inlineCode",{parentName:"p"},"numpy.ndarray")," object, it can be used to initialize a Starwhale Image object."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"import numpy\nfrom starwhale import Image\n\n# generate a random image numpy.ndarray\nrandom_array = numpy.random.randint(low=0, high=256, size=(100, 100, 3), dtype=numpy.uint8)\nimg = Image(random_array)\nprint(img)\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-console"},"ArtifactType.Image, display:, mime_type:MIMEType.UNDEFINED, shape:[None, None, 3], encoding:\n")),(0,r.kt)("h2",{id:"huggingface-datasets"},"Huggingface Datasets"),(0,r.kt)("p",null,"There are numerous datasets on the Huggingface Hub that can be transformed into Starwhale datasets with a single line of code."),(0,r.kt)("admonition",{type:"tip"},(0,r.kt)("p",{parentName:"admonition"},"Huggingface Datasets conversion relies on the ",(0,r.kt)("a",{parentName:"p",href:"https://pypi.org/project/datasets/"},"datasets")," library.")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'from starwhale import Dataset\n\n# You only specify starwhale dataset expected name and huggingface repo name\n# example: https://huggingface.co/datasets/lambdalabs/pokemon-blip-captions\nds = Dataset.from_huggingface("pokemon", "lambdalabs/pokemon-blip-captions")\nprint(ds)\nprint(len(ds))\nprint(repr(ds.fetch_one()))\n')),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-console"},"\ud83c\udf0a creating dataset local/project/self/dataset/pokemon/version/r2m3is6ormwcio4gtayop25qk4gmfr6mcei6hise...\n\ud83e\udd8b update 833 records into dataset\nDataset: pokemon, stash version: r2m3is6ormwcio4gtayop25qk4gmfr6mcei6hise, loading version: r2m3is6ormwcio4gtayop25qk4gmfr6mcei6hise\n833\nindex:default/train/0, features:{'image': ArtifactType.Image, display:, mime_type:MIMEType.JPEG, shape:[1280, 1280, 3], encoding: , 'text': 'a drawing of a green pokemon with red eyes', '_hf_subset': 'default', '_hf_split': 'train'}, shadow dataset: None\n")),(0,r.kt)("h2",{id:"pytorch"},"Pytorch"),(0,r.kt)("p",null,"Starwhale Dataset can be converted into Pytorch's ",(0,r.kt)("a",{parentName:"p",href:"https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset"},"torch.utils.dataset.IterableDataset")," object and accept transform. The converted Pytorch dataset object can then be passed to Pytorch dataloader or Huggingface Trainer, etc."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'from starwhale import dataset\nimport torch.utils.data as tdata\n\ndef custom_transform(data):\n    data["label"] = data["label"] + 100\n    return data\n\nwith dataset("simple", create="empty") as ds:\n    for i in range(0, 10):\n        ds[i] = {"text": f"{i}-text", "label": i}\n    ds.commit()\n\n    torch_ds = ds.to_pytorch(transform=custom_transform)\n    torch_loader = tdata.DataLoader(torch_ds, batch_size=1)\n    item = next(iter(torch_loader))\n    print(item)\n    print(item["label"])\n')),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-console"},"{'text': ['0-text'], 'label': tensor([100])}\ntensor([100])\n")),(0,r.kt)("h2",{id:"tensorflow"},"Tensorflow"),(0,r.kt)("p",null,"Starwhale Dataset can be converted into Tensorflow's ",(0,r.kt)("a",{parentName:"p",href:"https://www.tensorflow.org/api_docs/python/tf/data/Dataset"},"tensorflow.data.Dataset")," object and supports transform functions to mutate the data."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'from starwhale import dataset\n\n# login cloud instance in advance: `swcli instance login` command or `starwhale.login` sdk\n# raw dataset url: https://cloud.starwhale.cn/projects/397/datasets/172/versions/236/files\nds = dataset("https://cloud.starwhale.cn/project/starwhale:helloworld/dataset/mnist64")\ntf_ds = ds.to_tensorflow()\nprint(tf_ds)\n')),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-console"},"<_FlatMapDataset element_spec={'label': TensorSpec(shape=(), dtype=tf.int64, name=None), 'img': TensorSpec(shape=(8, 8, 1), dtype=tf.uint8, name=None)}>\n")))}c.isMDXComponent=!0}}]);